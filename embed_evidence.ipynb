{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32ef1366",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install loguru"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19838290",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "print(os.getcwd())\n",
    "sys.path.append('Sim_tools')\n",
    "\n",
    "from Sim_tools.dataset_Sim import TrainDataset, TestDataset\n",
    "from Sim_tools.model_Sim import SimcseModel, simcse_sup_loss, simcse_unsup_loss\n",
    "from Sim_tools.train_Sim import load_train_data_supervised, train_sup\n",
    "from Sim_tools.embed_evidence import embed_evidence\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import BertModel, BertConfig, BertTokenizer\n",
    "from loguru import logger\n",
    "from tqdm import tqdm\n",
    "\n",
    "import pandas as pd\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b27cef1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 16\n",
    "max_length = 256\n",
    "device = (\n",
    "    \"cuda\"\n",
    "    if torch.cuda.is_available()\n",
    "    else \"mps\" if torch.backends.mps.is_available() else \"cpu\"\n",
    ")\n",
    "\n",
    "print(f\"Using device: {device}\")\n",
    "checkpoint = \"bert-base-uncased\"\n",
    "tokenizer = BertTokenizer.from_pretrained(checkpoint)\n",
    "model = SimcseModel(pretrained_model=checkpoint, pooling='pooler', dropout=0.1).to(device)\n",
    "model.load_state_dict(torch.load(\"saved_model/best_model.pt\", map_location=device))\n",
    "model.eval()\n",
    "\n",
    "    \n",
    "evidence_csv_path = \"data/evidence.json\"  \n",
    "output_csv_path = \"data/evidence_embed.json\" \n",
    "embed_evidence(evidence_csv_path, output_csv_path, model, tokenizer, device, max_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58a5aae3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def match_evidence_by_similarity(claim_embedding, evidence_embeddings_dict, top_k=5, temperature=0.05):\n",
    "    \"\"\"\n",
    "    根据 claim 与 evidence embedding 的相似度，返回最相关的 evidence ID 列表。\n",
    "\n",
    "    参数:\n",
    "        claim_embedding: torch.Tensor，形状为 [768]（或其他维度）\n",
    "        evidence_embeddings_dict: dict，格式 {'evidence-id': torch.Tensor([768])}\n",
    "        top_k: 返回的 evidence 数量\n",
    "        temperature: softmax 温度缩放因子\n",
    "\n",
    "    返回:\n",
    "        List[str]：与 claim 最相关的 evidence id（按相似度排序）\n",
    "    \"\"\"\n",
    "\n",
    "    # 所有 evidence 的 ID 和向量堆叠成矩阵\n",
    "    evidence_ids = list(evidence_embeddings_dict.keys())\n",
    "    evidence_tensor = torch.stack([evidence_embeddings_dict[eid] for eid in evidence_ids])  # [num_evidence, 768]\n",
    "\n",
    "    # 计算余弦相似度\n",
    "    sim_scores = F.cosine_similarity(claim_embedding.unsqueeze(0), evidence_tensor, dim=1)  # [num_evidence]\n",
    "\n",
    "    # softmax 转成概率（可选，如果你只想排序，不一定要 softmax）\n",
    "    sim_probs = F.softmax(sim_scores / temperature, dim=0)  # [num_evidence]\n",
    "\n",
    "    # 取 top-k\n",
    "    topk_probs, topk_indices = torch.topk(sim_probs, top_k)\n",
    "\n",
    "    # 返回 evidence id（按相似度高到低排序）\n",
    "    top_evidence_ids = [evidence_ids[i] for i in topk_indices]\n",
    "\n",
    "    return top_evidence_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b3f945b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def varify_evidence(train_json_path, evidence_embeddings_dict, top_k=5,temperature=0.05):\n",
    "\n",
    "\n",
    "    train_json_path = \"data/train-claims.json\"\n",
    "    with open(train_json_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        train_data = json.load(f)\n",
    "\n",
    "    for claim_id, claim_info in train_data.items():\n",
    "\n",
    "        claim_text = claim_info[\"claim_text\"]\n",
    "        positive_ids = claim_info[\"evidences\"]\n",
    "        \n",
    "        inputs = tokenizer(\n",
    "            claim_text,\n",
    "            max_length=max_length,\n",
    "            truncation=True,\n",
    "            padding='max_length',\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "\n",
    "        input_ids = inputs['input_ids'].to(device)\n",
    "        attention_mask = inputs['attention_mask'].to(device)\n",
    "        token_type_ids = inputs['token_type_ids'].to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            claim_embedding = model(input_ids, attention_mask, token_type_ids)        \n",
    "        result_lst = match_evidence_by_similarity(claim_embedding, evidence_embeddings_dict, top_k=5, temperature=0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe82f621",
   "metadata": {},
   "outputs": [],
   "source": [
    "def varify_evidence(train_json_path, evidence_embeddings_dict, model, tokenizer, device, max_length=256, top_k=5, temperature=0.05):\n",
    "\n",
    "    with open(train_json_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        train_data = json.load(f)\n",
    "\n",
    "    total_claims = 0\n",
    "    total_hits = 0\n",
    "\n",
    "    for claim_id, claim_info in train_data.items():\n",
    "        claim_text = claim_info[\"claim_text\"]\n",
    "        positive_ids = set(claim_info[\"evidences\"])  # ground truth ids as set\n",
    "\n",
    "        # Tokenize claim\n",
    "        inputs = tokenizer(\n",
    "            claim_text,\n",
    "            max_length=max_length,\n",
    "            truncation=True,\n",
    "            padding='max_length',\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        input_ids = inputs['input_ids'].to(device)\n",
    "        attention_mask = inputs['attention_mask'].to(device)\n",
    "        token_type_ids = inputs['token_type_ids'].to(device)\n",
    "\n",
    "        # Get claim embedding\n",
    "        with torch.no_grad():\n",
    "            claim_embedding = model(input_ids, attention_mask, token_type_ids)  # [1, 768]\n",
    "            claim_embedding = claim_embedding.squeeze(0)  # -> [768]\n",
    "\n",
    "        # Get top-k matching evidence ids\n",
    "        result_lst = match_evidence_by_similarity(claim_embedding, evidence_embeddings_dict, top_k=top_k, temperature=temperature)\n",
    "\n",
    "        # Evaluate hit (if any of top-k is in positive ids)\n",
    "        hit = any(eid in positive_ids for eid in result_lst)\n",
    "        total_hits += int(hit)\n",
    "        total_claims += 1\n",
    "\n",
    "    accuracy = total_hits / total_claims if total_claims > 0 else 0.0\n",
    "    print(f\"Top-{top_k} Accuracy: {accuracy:.4f} ({total_hits}/{total_claims})\")\n",
    "    return accuracy"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
